---
title: மாயக்காட்சி
description: நம்பத்தகுந்ததாகத் தோன்றினாலும் உண்மையில் தவறான அல்லது கற்பனையான AI வெளியீடுகள்
---

மாயக்காட்சி (hallucination) என்பது தன்னம்பிக்கையுடனும் ஒழுங்காகவும் வாசிக்கப்படினும், உண்மைப் பிழைகள், கற்பனையாக உருவாக்கப்பட்ட தகவல்கள், அல்லது கண்டுபிடிக்கப்பட்ட மேற்கோள்கள் ஆகியவற்றைக் கொண்ட AI வெளியீடு. இங்கு ஆபத்து சீரற்ற தன்மையல்ல: *நம்பத்தகுந்த தோற்றம்* தான். மாயக்காட்சியான உரை, உண்மையான நடைமுறை எழுத்தின் புள்ளியியல் வடிவங்களைப் பின்பற்றுவதால், சாதாரண மேம்போக்கு சரிபார்ப்பை எளிதில் கடந்து விடும்.

மாயக்காட்சி என்பது [LLMs](./llm) எவ்வாறு இயங்குகின்றன என்பதின் ஒரு அறிகுறி: அவை உண்மையான அடுத்த “டோக்கன்”களை அல்ல, அதிக வாய்ப்புள்ள அடுத்த டோக்கன் (token)களை முன்னறிவிக்கின்றன. ஒரு தலைப்பில் பயிற்சித் தரவு குறைவாகவோ அல்லது முரண்பாடுகளோடு இருந்தாலோ, மாதிரி (model) இடைநிரப்பு (interpolate) செய்கிறது; அதன் விளைவாக மென்மையாகத் தவறான முடிவு வரக்கூடும். இதனால்தான் [fluency trap](./fluency-trap) மிக ஆபத்தானது: சொல்வன்மை (fluency) துல்லியத்தைக் (accuracy) குறிக்காது.

[ Coscientist](./coscientist) இல், மாயக்காட்சி அபாயம் [epistemic protocol layer](./epistemic-protocol-layer) மூலம் நிர்வகிக்கப்படுகிறது: [traceability](./traceability) என்பது கூற்றுகள் [evidence spans](./evidence-span) உடன் இணைவதை வேண்டுகிறது, [rebuttal-first search](./rebuttal-first-search) ஏற்றுக்கொள்வதற்கு முன் அழுத்தச் சோதனை (stress-test) செய்கிறது, மேலும் [Multi-AI Consensus Protocol](./multi-ai-consensus-protocol) மாதிரிகளிடையேயான முரண்பாட்டை (model disagreement) நெருக்கமான பரிசோதனைக்கான ஒரு சிக்னலாகப் பயன்படுத்துகிறது.