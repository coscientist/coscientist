---
title: 환각(Hallucination)
description: 그럴듯하지만 사실과 다르거나 조작된 AI 출력
---

환각(hallucination)은 확신에 차고 일관되게 읽히지만 사실 오류, 조작된 정보, 또는 꾸며낸 출처를 포함하는 AI 출력입니다. 위험은 무작위성이 아니라 그럴듯함입니다. 환각된 텍스트는 진실한 산문의 통계적 패턴을 따르기 때문에, 대충 훑어보는 수준의 검증은 종종 통과합니다.

환각은 [LLMs](./llm)가 작동하는 방식의 증상입니다. 즉, 사실인 다음 토큰이 아니라 그럴 가능성이 높은 다음 토큰을 예측합니다. 어떤 주제에 대해 학습 데이터가 희박하거나 상충될 때 모델은 보간(interpolation)하며, 그 결과는 매끄럽게 틀릴 수 있습니다. 이것이 [유창성 함정](./fluency-trap)이 그토록 위험한 이유입니다. 유창함은 정확성을 의미하지 않습니다.

[코사이언티스트(Coscientist)](./coscientist)에서는 [인식론적 프로토콜 계층](./epistemic-protocol-layer)을 통해 환각 위험을 관리합니다. [추적 가능성](./traceability)은 주장들이 [증거 구간](./evidence-span)과 연결되도록 요구하고, [반박 우선 탐색](./rebuttal-first-search)은 수용 전에 스트레스 테스트를 수행하며, [Multi-AI 합의 프로토콜](./multi-ai-consensus-protocol)은 모델 간 불일치를 더 면밀한 검토가 필요하다는 신호로 사용합니다.