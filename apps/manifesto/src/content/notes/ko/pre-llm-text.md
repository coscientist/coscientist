---
title: Pre-LLM 텍스트
description: 대규모 언어 모델이 웹을 오염시키기 전, 사람이 직접 작성한 콘텐츠
sourceLocale: en
sourceHash: 45aaafa47592
translatedAt: 2026-01-14
---

Pre-LLM 텍스트는 대규모 언어 모델이 그럴듯한 산문을 대규모로 생성할 수 있게 되기 전—대략 2020년 이전—에 사람이 직접 작성한 콘텐츠를 말합니다. 그 가치는 검증된 [출처(프로비넌스, provenance)](./provenance)에 있습니다. 즉, 합성물이 아니고, AI 출력에서 재귀적으로 파생된 것도 아닌, 명백한 인간 작성물입니다.

[저배경 강철](./low-background-steel)에 대한 비유는 최근 담론에서 명시적으로 등장합니다. 핵실험이 1945년 이후 생산된 모든 강철을 방사성 동위원소로 오염시켰듯이, LLM의 확산은 웹을 인간 글쓰기와 구분되지 않는 합성 텍스트로 오염시켰습니다. Pre-LLM 텍스트는 [오염 이전 자원](./pre-contamination-resource)입니다. 한정되어 있고, 대체 불가능하며, 시간이 갈수록 더 가치가 커집니다.

위험은 실질적입니다. AI 생성 텍스트로 AI를 학습시키면 [모델 붕괴](./model-collapse)가 발생합니다. 출력 분포가 좁아지고, 드문 모드는 사라지며, 세대를 거듭할수록 품질이 저하됩니다. 이 재귀적 함정을 피하려면 깨끗한 인간 작성 코퍼스가 필수지만, [학습 데이터 오염](./training-data-contamination) 때문에 2020년 이후 웹 스크레이프는 신뢰하기 어렵습니다. 검증 가능한 타임스탬프를 갖춘 아카이브—Internet Archive, 학술 리포지터리, 2020년 이전 Common Crawl 스냅샷—가 향후 학습을 위한 1차 자료가 됩니다.

Pre-LLM 텍스트는 연구 타당성에도 중요합니다. 인간 언어, 인지, 문화에 대한 연구는 이전 모델로부터 학습된 통계적 패턴이 아니라 인간의 생산을 반영하는 것으로 알려진 코퍼스를 필요로 합니다. 웹을 범람하는 [AI 슬롭](./ai-slop)은 모든 다운스트림 분석의 노이즈 플로어를 높입니다.

아이러니는 저배경 강철과 닮아 있습니다. 가장 발전한 AI 시스템일수록, AI가 글을 쓸 만큼 발전하기 전에 생산된 텍스트에 의존하게 될 것입니다.
