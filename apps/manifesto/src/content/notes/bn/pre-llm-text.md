---
title: প্রি-LLM টেক্সট
description: বড় ভাষা মডেলগুলো ওয়েবকে দূষিত করার আগে মানুষের লেখা কনটেন্ট
sourceLocale: en
sourceHash: 45aaafa47592
translatedAt: 2026-01-14
---

প্রি-LLM টেক্সট হলো মানুষের লেখা কনটেন্ট, যা বড় ভাষা মডেল ব্যাপক পরিসরে বিশ্বাসযোগ্য গদ্য তৈরি করতে সক্ষম হওয়ার আগেই তৈরি—মোটামুটি ২০২০ সালের আগে। এর মূল্য নিহিত যাচাইকৃত [উৎপত্তি](./provenance)-তে: এটি সন্দেহাতীতভাবে মানুষ-লিখিত, কৃত্রিম নয়, এবং AI আউটপুট থেকে পুনরাবৃত্তিমূলকভাবে উদ্ভূতও নয়।

সাম্প্রতিক আলোচনায় [লো-ব্যাকগ্রাউন্ড স্টিল](./low-background-steel)-এর সঙ্গে উপমাটি স্পষ্ট। যেমন পারমাণবিক পরীক্ষার কারণে ১৯৪৫-পরবর্তী সব স্টিল তেজস্ক্রিয় আইসোটোপে দূষিত হয়ে পড়েছিল, তেমনি LLM-এর বিস্তারের ফলে ওয়েব কৃত্রিম টেক্সটে দূষিত হয়েছে, যা মানুষের লেখার থেকে আলাদা করা যায় না। প্রি-LLM টেক্সট একটি [দূষণ-পূর্ব সম্পদ](./pre-contamination-resource): সীমিত, প্রতিস্থাপন-অযোগ্য, এবং ক্রমেই বেশি মূল্যবান।

ঝুঁকিগুলো বাস্তব। AI-উৎপাদিত টেক্সটে AI-কে প্রশিক্ষণ দিলে [মডেল ধস](./model-collapse) ঘটে: আউটপুট ডিস্ট্রিবিউশন সংকীর্ণ হয়, বিরল মোড বিলুপ্ত হয়, এবং প্রজন্মের পর প্রজন্মে গুণগত মান কমতে থাকে। এই পুনরাবৃত্ত ফাঁদ এড়াতে পরিচ্ছন্ন মানুষ-লিখিত করপাস অপরিহার্য, কিন্তু [প্রশিক্ষণ ডেটা দূষণ](./training-data-contamination) ২০২০-পরবর্তী ওয়েব স্ক্র্যাপকে অনির্ভরযোগ্য করে তোলে। যাচাইযোগ্য টাইমস্ট্যাম্পসহ আর্কাইভ—ইন্টারনেট আর্কাইভ, একাডেমিক রিপোজিটরি, ২০২০-পূর্ব কমন ক্রল স্ন্যাপশট—ভবিষ্যৎ প্রশিক্ষণের জন্য প্রাথমিক উৎস হয়ে ওঠে।

গবেষণার বৈধতার জন্যও প্রি-LLM টেক্সট গুরুত্বপূর্ণ। মানব ভাষা, জ্ঞান , এবং সংস্কৃতি নিয়ে গবেষণায় এমন করপাস দরকার, যা নিশ্চিতভাবে মানব উৎপাদনকে প্রতিফলিত করে—পূর্ববর্তী মডেল থেকে শেখা পরিসংখ্যানগত প্যাটার্ন নয়। ওয়েবে [AI স্লপ](./ai-slop) প্লাবিত হওয়ায় পরবর্তী সব বিশ্লেষণের জন্য নয়েজ ফ্লোর বেড়ে যায়।

বিরূপতা লো-ব্যাকগ্রাউন্ড স্টিলের মতোই: সবচেয়ে উন্নত AI সিস্টেমগুলো নির্ভর করবে এমন টেক্সটের ওপর, যা লেখা হয়েছিল AI এতটা উন্নত হওয়ার আগেই যে সে নিজে লিখতে পারে।
