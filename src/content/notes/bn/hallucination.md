---
title: হ্যালুসিনেশন
description: AI-এর এমন আউটপুট যা বিশ্বাসযোগ্য মনে হলেও বাস্তবভাবে ভুল বা বানানো
sourceLocale: en
sourceHash: 35a042b51f6a
translatedAt: 2026-01-14
---

হ্যালুসিনেশন হলো AI-এর এমন একটি আউটপুট, যা আত্মবিশ্বাসী ও সুসংগত বলে মনে হয়,
কিন্তু তাতে বাস্তবগত ভুল, বানানো তথ্য, বা উদ্ভাবিত রেফারেন্স থাকে। বিপদটি
এলোমেলোতা নয়: বিপদটি হলো বিশ্বাসযোগ্যতা। হ্যালুসিনেটেড লেখা অনেক সময় আনুষ্ঠানিক
নজরদারি ছাড়াই যাচাইয়ে টিকে যায়, কারণ এটি সত্যনিষ্ঠ গদ্যের পরিসংখ্যানগত প্যাটার্ন
অনুসরণ করে।

হ্যালুসিনেশন হলো [বৃহৎ ভাষা মডেলসমূহ](./llm) কীভাবে কাজ করে তার একটি উপসর্গ:
এগুলো "সত্য" টোকেন নয়, বরং সম্ভাব্য পরবর্তী টোকেন অনুমান করে। কোনো বিষয়ের ওপর
ট্রেনিং ডেটা যদি কম থাকে বা পরস্পরবিরোধী হয়, মডেল তখন মাঝামাঝি করে অনুমান করে,
এবং ফলাফল হতে পারে মসৃণভাবে ভুল। এ কারণেই [fluency trap](./fluency-trap) এত
বিপজ্জনক: সাবলীলতা মানেই নির্ভুলতা নয়।

[সহ-বিজ্ঞানী](./coscientist)-এ, হ্যালুসিনেশনের ঝুঁকি নিয়ন্ত্রণ করা হয়
[epistemic protocol layer](./epistemic-protocol-layer)-এর মাধ্যমে:
[traceability](./traceability) দাবি -গুলোকে [evidence spans](./evidence-span)-এর
সঙ্গে যুক্ত করতে বাধ্য করে,
[খণ্ডন-প্রাথমিকতা অনুসন্ধান](./rebuttal-first-search) গ্রহণের আগে চাপ-পরীক্ষা
চালায়, এবং [Multi-AI Consensus Protocol](./multi-ai-consensus-protocol)
মডেলগুলোর মতানৈক্যকে আরও কাছ থেকে পর্যালোচনার সংকেত হিসেবে ব্যবহার করে।
