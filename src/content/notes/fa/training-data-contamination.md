---
title: آلودگی داده‌های آموزشی
description: محتوای تولیدشده توسط هوش مصنوعی که پیکره‌های مورد استفاده برای آموزش مدل‌های آینده را آلوده می‌کند
sourceLocale: en
sourceHash: a873d340b329
translatedAt: 2026-01-14
---

آلودگی داده‌های آموزشی زمانی رخ می‌دهد که متنِ تولیدشده توسط هوش مصنوعی وارد وب می‌شود،
بعداً در پیکره‌های آموزشی گردآوری می‌شود و نسل بعدی مدل‌ها را شکل می‌دهد.
نتیجه یک حلقهٔ بازخورد است: مدل‌هایی که بر خروجی‌های پیشینیان خود آموزش می‌بینند،
سوگیری‌های آن‌ها را به ارث می‌برند، خطاهایشان را تشدید می‌کنند، و دسترسی به سیگنال مستقلِ انسانی را
که نسخه‌های اولیه را مفید کرده بود از دست می‌دهند.

این پدیده با آلودگی بنچمارک (نشت داده‌های آزمون به مجموعه‌های آموزشی) متفاوت است،
هرچند هر دو از یک واژه استفاده می‌کنند. آلودگی داده‌های آموزشی دربارهٔ منشأ پیکرهٔ زیربنایی است:
وقتی [مزخرفات هوش مصنوعی](./ai-slop) در مقیاس بزرگ با متنِ نوشته‌شده توسط انسان درهم می‌آمیزد،
تمایز دادنشان پرهزینه یا ناممکن می‌شود.
گردآوری‌های وب پس از ۲۰۲۲ هرچه بیشتر مشکوک‌اند.

پیامدها روی هم انباشته می‌شوند. [فروپاشی مدل](./model-collapse) افت کیفیت را توصیف می‌کند وقتی
مدل‌ها بر دادهٔ مصنوعی آموزش می‌بینند: توزیع‌ها باریک می‌شوند،
حالت‌های نادر ناپدید می‌شوند، و خروجی به‌سوی میانگینی همگن همگرا می‌شود.
[فروپاشی دانشنامه](./encyclopedia-meltdown) شکستِ نظام دانشی را توصیف می‌کند وقتی
خروجی‌های هوش مصنوعی به‌عنوان منبع ارجاع داده می‌شوند و اقتداری دوری ایجاد می‌کنند.
آلودگی داده‌های آموزشی علت بالادستیِ هر دو است.

قیاس با [فولاد کم‌پس‌زمینه](./low-background-steel) مسئله را روشن می‌کند.
آزمایش‌های هسته‌ای همهٔ فولادِ پس از ۱۹۴۵ را آلوده کرد؛ تکثیر مدل‌های زبانی بزرگ
همهٔ متنِ وبِ پس از ۲۰۲۰ را آلوده کرد. هر دو رویدادِ آلودگی برگشت‌ناپذیر بودند،
هر دو برای [منابعِ پیشا-آلودگی](./pre-contamination-resource) تقاضا ایجاد کردند،
و هر دو به این معنا هستند که پیشبرد فناوری به موادی نیاز دارد که پیش از وجودِ خودِ فناوری تولید شده‌اند.

راه‌حل‌ها شامل راستی‌آزماییِ [منشأ](./provenance)،
آرشیوهای دارای دروازهٔ زمانی ،
و شیوه‌های پالایش داده است که منابع دارای زنجیره‌های روشنِ نویسندگی انسانی را در اولویت می‌گذارند.
ابتکار منشأ دادهٔ MIT و تلاش‌های مشابه می‌کوشند
شفافیت را به منشأ داده‌های آموزشی بیاورند—گامی ضروری اگر قرار است مدل‌های آینده
از آموزش دیدن بر بازتاب‌های خودشان پرهیز کنند.
