---
title: హాల్యూసినేషన్
description: నమ్మదగినట్టు కనిపించినా వాస్తవంగా తప్పు లేదా కల్పితమైన AI అవుట్‌పుట్లు
sourceLocale: en
sourceHash: 35a042b51f6a
translatedAt: 2026-01-14
---

హాల్యూసినేషన్ అనేది నమ్మకం కలిగించేలా, సుసంపన్నంగా చదువబడినా వాస్తవపరమైన
పొరపాట్లు, కల్పిత సమాచారం, లేదా ఊహించిన సూచనలు ఉన్న AI అవుట్‌పుట్. ప్రమాదం
యాదృచ్ఛికత కాదు: నమ్మదగినట్టు కనిపించడమే. హాల్యూసినేట్ చేసిన పాఠ్యం తరచూ సాధారణ
పరిశీలనను దాటిపోతుంది, ఎందుకంటే అది నిజమైన గద్యంలోని గణాంక నమూనాలను
అనుసరిస్తుంది.

హాల్యూసినేషన్ అనేది [ఎల్ఎల్ఎమ్‌లు](./llm) ఎలా పనిచేస్తాయో దానికి సంబంధించిన
లక్షణం: అవి నిజమైన టోకెన్లను కాదు, తదుపరి వచ్చే అవకాశమున్న టోకెన్లను అంచనా
వేస్తాయి. ఏదైనా అంశంపై శిక్షణ డేటా అరుదుగా లేదా పరస్పర విరుద్ధంగా ఉన్నప్పుడు,
మోడల్ మధ్యవర్తిత్వం చేస్తుంది—దాంతో ఫలితం మృదువుగా ఉన్నా తప్పుగా ఉండొచ్చు.
అందుకే [fluency trap](./fluency-trap) అంత ప్రమాదకరం: ప్రవాహం ఉండటం
ఖచ్చితత్వాన్ని సూచించదు.

[కోషియన్‌టిస్ట్](./coscientist) లో, హాల్యూసినేషన్ ప్రమాదాన్ని
[ఎపిస్టెమిక్ ప్రోటోకాల్ పొర](./epistemic-protocol-layer) ద్వారా నియంత్రిస్తారు:
[traceability](./traceability) లో దావాలు [evidence spans](./evidence-span) తో
అనుసంధానమై ఉండాలి, [ఖండన-మొదట శోధన](./rebuttal-first-search) అంగీకరించే ముందు
కఠినంగా పరీక్షిస్తుంది, మరియు
[బహు-AI ఏకాభిప్రాయ ప్రోటోకాల్](./multi-ai-consensus-protocol) మోడళ్ల మధ్య
అసమ్మతిని మరింత సన్నిహిత పరిశీలనకు సంకేతంగా ఉపయోగిస్తుంది.
