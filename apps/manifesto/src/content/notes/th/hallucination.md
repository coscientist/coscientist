---
title: อาการหลอน (Hallucination)
description: ผลลัพธ์จาก AI ที่ดูสมเหตุสมผลแต่ไม่ถูกต้องตามข้อเท็จจริงหรือถูกแต่งขึ้น
---

อาการหลอน (hallucination) คือผลลัพธ์จาก AI ที่อ่านแล้วดูมั่นใจและสอดคล้องต่อเนื่อง แต่มีข้อผิดพลาดเชิงข้อเท็จจริง ข้อมูลที่ถูกแต่งขึ้น หรือการอ้างอิงที่สร้างขึ้นเอง อันตรายไม่ใช่ความสุ่ม แต่คือ "ความน่าเชื่อ” ข้อความที่หลอนมักผ่านการตรวจแบบผิวเผินได้ เพราะมันสอดคล้องกับรูปแบบเชิงสถิติของร้อยแก้วที่เป็นความจริง

อาการหลอนเป็นอาการสะท้อนวิธีการทำงานของ [โมเดลภาษาขนาดใหญ่](./llm): พวกมันคาดเดาโทเคนถัดไปที่ "น่าจะเป็นไปได้” ไม่ใช่โทเคนที่ "เป็นความจริง” เมื่อข้อมูลฝึกมีน้อยหรือขัดแย้งกันในหัวข้อหนึ่ง โมเดลจะอนุมานเติมช่องว่าง (interpolate) และผลลัพธ์อาจ "ผิดได้อย่างลื่นไหล” นี่คือเหตุผลที่ [กับดักความลื่นไหล](./fluency-trap) อันตรายมาก: ความลื่นไหลไม่ได้หมายความว่าถูกต้อง

ใน [นักวิทยาศาสตร์ร่วม](./coscientist) ความเสี่ยงจากอาการหลอนถูกจัดการผ่าน [ชั้นโปรโตคอลเชิงญาณวิทยา](./epistemic-protocol-layer): [การตรวจสอบย้อนกลับ](./traceability) กำหนดให้ข้ออ้างต้องเชื่อมโยงกับ [ช่วงหลักฐาน](./evidence-span), [การค้นหาแบบโต้แย้งก่อน (rebuttal-first search)](./rebuttal-first-search) ใช้ทดสอบความแข็งแรงก่อนยอมรับ และ [โปรโตคอลฉันทามติหลาย AI](./multi-ai-consensus-protocol) ใช้ความเห็นไม่ตรงกันของโมเดลเป็นสัญญาณว่าควรตรวจสอบอย่างใกล้ชิด
