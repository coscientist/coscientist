---
title: Pre-LLM 텍스트
description: 대규모 언어 모델의 웹 오염 이전에 생산된 인간 저작 콘텐츠
sourceLocale: en
sourceHash: 45aaafa47592
translatedAt: 2026-01-15
---

Pre-LLM 텍스트란 대규모 언어 모델이 그럴듯한 산문을 대량으로 생성할 수 있게 되기 이전—대략 2020년 이전—에 인간이 저작한 콘텐츠를 지칭한다. 그 가치는 검증된 [출처](./provenance)에 있다. 즉, 합성물이 아니며, AI 출력물로부터 재귀적으로 파생된 것이 아닌, 명백한 인간 저작물이라는 점이다.

[저배경강](./low-background-steel)과의 유비는 최근 담론에서 명시적으로 제기되고 있다. 핵실험이 1945년 이후 생산된 모든 강철을 방사성 동위원소로 오염시켰듯이, LLM의 확산은 웹을 인간의 글쓰기와 식별 불가능한 합성 텍스트로 오염시켰다. Pre-LLM 텍스트는 [오염 이전 자원](./pre-contamination-resource)에 해당한다. 그것은 유한하고, 대체 불가능하며, 시간이 경과할수록 그 가치가 증대한다.

이 문제의 이해관계는 실질적이다. AI 생성 텍스트로 AI를 훈련시킬 경우 [모델 붕괴](./model-collapse)가 발생한다. 출력 분포가 협소해지고, 희귀 모드가 소실되며, 세대를 거듭할수록 품질이 저하된다. 이러한 재귀적 함정을 회피하기 위해서는 오염되지 않은 인간 저작 코퍼스가 필수적이나, [훈련 데이터 오염](./training-data-contamination)으로 인해 2020년 이후의 웹 스크레이프는 신뢰성이 결여되어 있다. 검증 가능한 타임스탬프를 보유한 아카이브—Internet Archive, 학술 리포지터리, 2020년 이전 Common Crawl 스냅샷 등—가 향후 훈련을 위한 일차 자료로서 기능하게 된다.

Pre-LLM 텍스트는 연구의 타당성 측면에서도 중요하다. 인간 언어, 인지, 문화에 관한 연구는 선행 모델로부터 학습된 통계적 패턴이 아닌, 인간의 생산물임이 확인된 코퍼스를 요구한다. 웹에 범람하는 [AI 슬롭](./ai-slop)은 모든 하류 분석의 잡음 수준을 상승시킨다.

여기에는 저배경강과 유사한 아이러니가 존재한다. 가장 진보한 AI 시스템일수록, AI가 글을 작성할 수 있을 만큼 발전하기 이전에 생산된 텍스트에 의존하게 될 것이다.
