---
title: LLM
description: "Large Language Model, the AI architecture underlying Coscientist's contemplation labor"
---

LLM refers to neural network models trained on massive text corpora to predict and generate natural language. Examples include GPT, Claude, Gemini, and Llama. LLMs can perform a wide range of language tasks—summarization, translation, question-answering, code generation—by learning statistical patterns from training data.

For [Coscientist](./coscientist), LLMs are the engine that performs [contemplation labor](./contemplation-labor): proposing hypotheses, gathering evidence, finding counterexamples, and structuring arguments. Because LLMs can read any language, they enable [cross-linguistic synthesis](./cross-linguistic-synthesis) as a native capability.

However, LLMs have fundamental limitations. They optimize for plausible next tokens, not for truth. They can [hallucinate](./hallucination): producing confident, coherent text that is factually wrong. They are susceptible to the [fluency trap](./fluency-trap): smooth prose that masks errors. They share training data, so agreement among models may reflect correlated bias rather than independent [verification](./verification).

This is why [Coscientist](./coscientist) treats LLMs as tools, not oracles. The [Operator](./operator) retains sovereignty; the [epistemic protocol layer](./epistemic-protocol-layer) enforces [traceability](./traceability) and [rebuttal-first search](./rebuttal-first-search); and the [Multi-AI Consensus Protocol](./multi-ai-consensus-protocol) uses model disagreement as a signal for closer inspection. LLMs do the search and structuring; humans do the verification and decision.
