---
title: LLM
description: 大型语言模型（Large Language Model），支撑 Coscientist 思辨劳动的 AI 架构
sourceLocale: en
sourceHash: a03c9bb62d5e
translatedAt: 2026-01-14
---

LLM（Large Language
Model）指在海量文本语料上训练的神经网络模型，用于预测并生成自然语言。例子包括 GPT、Claude、Gemini 和 Llama。LLM 通过从训练数据中学习统计模式，能够执行广泛的语言任务——摘要、翻译、问答、代码生成等。

对于 [共研者](./coscientist)
而言，LLM 是执行[思辨劳动](./contemplation-labor)的引擎：提出假设、收集证据、寻找反例，并对论证进行结构化。由于 LLM 能读懂任何语言，它们将[跨语言综合](./cross-linguistic-synthesis)作为一种原生能力实现。

然而，LLM 存在根本性局限。它们优化的是"看起来合理的下一个 token"，而不是真实性。它们会[幻觉](./hallucination)：生成自信、连贯但事实错误的文本。它们也容易落入[流畅性陷阱](./fluency-trap)：顺滑的文笔掩盖了错误。它们共享训练数据，因此模型之间的一致可能反映的是相关性偏差，而非独立的[验证](./verification)（见[证据独立性](./evidence-independence)）。

这就是为什么 [共研者](./coscientist)
将 LLM 视为工具，而不是神谕。[操作员](./operator)保留主权；[认识论协议层](./epistemic-protocol-layer)强制[可追溯性](./traceability)与[反驳优先搜索](./rebuttal-first-search)；而[多 AI 共识协议](./multi-ai-consensus-protocol)将模型分歧作为需要更细致审查的信号。LLM 负责搜索与结构化；人类负责验证与决策。
