---
title: Pre-LLM-Text
description: Von Menschen verfasste Inhalte, die entstanden sind, bevor große Sprachmodelle das Web kontaminierten
sourceLocale: en
sourceHash: 45aaafa47592
translatedAt: 2026-01-14
---

Pre-LLM-Text ist von Menschen verfasster Inhalt, der entstanden ist, bevor große Sprachmodelle in der Lage waren, in großem Maßstab plausibel klingende Prosa zu erzeugen – grob gesagt vor 2020. Sein Wert liegt in der verifizierten [Herkunft](./provenance): Er ist eindeutig menschlich geschrieben, nicht synthetisch und nicht rekursiv aus KI-Outputs abgeleitet.

Die Analogie zu [low-background steel](./low-background-steel) ist in der jüngeren Debatte ausdrücklich. So wie Atomtests sämtlichen Stahl nach 1945 mit radioaktiven Isotopen kontaminierten, hat die Verbreitung von LLMs das Web mit synthetischem Text kontaminiert, der von menschlichem Schreiben nicht zu unterscheiden ist. Pre-LLM-Text ist eine [Ressource vor der Kontamination](./pre-contamination-resource): endlich, nicht ersetzbar und zunehmend wertvoll.

Was auf dem Spiel steht, ist ganz praktisch. KI mit KI-generiertem Text zu trainieren, verursacht [Model Collapse](./model-collapse): Ausgabeverteilungen verengen sich, seltene Modi verschwinden, und die Qualität nimmt über Generationen hinweg ab. Saubere, von Menschen geschriebene Korpora sind essenziell, um dieser rekursiven Falle zu entgehen, aber [Kontamination von Trainingsdaten](./training-data-contamination) macht Web-Scrapes nach 2020 unzuverlässig. Archive mit überprüfbaren Zeitstempeln – das Internet Archive, akademische Repositorien, Pre-2020-Snapshots von Common Crawl – werden zu Primärquellen für zukünftiges Training.

Pre-LLM-Text ist auch für die Validität von Forschung wichtig. Studien zu menschlicher Sprache, Kognition und Kultur benötigen Korpora, von denen bekannt ist, dass sie menschliche Produktion widerspiegeln – nicht statistische Muster, die aus früheren Modellen gelernt wurden. Das [AI slop](./ai-slop), das das Web überschwemmt, erhöht den „Noise Floor“ für jede nachgelagerte Analyse.

Die Ironie spiegelt die von low-background steel: Die fortschrittlichsten KI-Systeme werden von Text abhängen, der produziert wurde, bevor KI fortgeschritten genug war, um zu schreiben.
