---
title: LLM以前のテキスト
description: 大規模言語モデルがウェブを汚染する前に作成された、人間が書いたコンテンツ
sourceLocale: en
sourceHash: 45aaafa47592
translatedAt: 2026-01-14
---

LLM以前のテキストとは、大規模言語モデルがもっともらしい文章を大規模に生成できるようになる以前――おおむね2020年以前――に作成された、人間が著したコンテンツを指す。その価値は、検証可能な[出自](./provenance)にある。すなわち、それが紛れもなく人間の手によるものであり、合成（synthetic）でもなく、AI出力から再帰的に派生したものでもないという点だ。

[低バックグラウンド鋼](./low-background-steel)との類比は、近年の議論において明示的に語られている。核実験によって1945年以降のあらゆる鋼材が放射性同位体で汚染されたのと同様に、LLMの普及は、人間の文章と区別できない合成テキストでウェブを汚染してきた。LLM以前のテキストは[汚染前の資源](./pre-contamination-resource)である。有限で、代替不可能で、そしてますます価値が高まっている。

重要性は実務的だ。AI生成テキストでAIを学習させると[モデル崩壊](./model-collapse)が生じる。出力分布は狭まり、希少なモードは消失し、世代をまたぐにつれて品質が劣化する。この再帰的な罠を避けるには、クリーンな人間執筆コーパスが不可欠だが、[学習データ汚染](./training-data-contamination)により、2020年以降のウェブスクレイプは信頼できなくなっている。検証可能なタイムスタンプを備えたアーカイブ――Internet Archive、学術リポジトリ、2020年以前のCommon Crawlスナップショット――が、将来の学習における一次資料となる。

LLM以前のテキストは、研究の妥当性にとっても重要である。人間の言語、認知、文化を扱う研究には、先行モデルから学習された統計的パターンではなく、人間の産出を反映していると分かっているコーパスが必要だ。ウェブを氾濫する[AIスロップ](./ai-slop)は、あらゆる下流分析のノイズフロアを引き上げている。

この皮肉は低バックグラウンド鋼と響き合う。最先端のAIシステムほど、AIが文章を書けるほど十分に高度になる前に生み出されたテキストに依存することになる。
