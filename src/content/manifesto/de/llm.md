---
title: LLM
description: "Large Language Model, die KI-Architektur, die Coscientists Kontemplationsarbeit zugrunde liegt"
sourceLocale: en
sourceHash: a03c9bb62d5e
translatedAt: 2026-01-14
---

LLM bezeichnet neuronale Netzwerkmodelle, die auf riesigen Textkorpora trainiert
werden, um natürliche Sprache vorherzusagen und zu generieren. Beispiele sind
GPT, Claude, Gemini und Llama. LLMs können eine breite Palette von
Sprachaufgaben ausführen—Zusammenfassung, Übersetzung, Frage-Antwort,
Code-Generierung—indem sie statistische Muster aus Trainingsdaten lernen.

Für [Kozientist](./coscientist) sind LLMs der Motor, der
[Kontemplationsarbeit](./contemplation-labor) leistet: Hypothesen vorschlagen,
Belege sammeln, Gegenbeispiele finden und Argumente strukturieren. Da LLMs jede
Sprache lesen können, ermöglichen sie
[sprachübergreifende Synthese](./cross-linguistic-synthesis) als native
Fähigkeit.

LLMs haben jedoch grundlegende Einschränkungen. Sie optimieren auf plausible
nächste Tokens, nicht auf Wahrheit. Sie können [halluzinieren](./hallucination):
selbstbewussten, kohärenten Text produzieren, der faktisch falsch ist. Sie sind
anfällig für die [Flüssigkeitsfalle](./fluency-trap): geschmeidige Prosa, die
Fehler verdeckt. Sie teilen Trainingsdaten, daher kann Übereinstimmung zwischen
Modellen eher korrelierte Verzerrung als unabhängige
[Verifikation](./verification) widerspiegeln (siehe
[Evidenzunabhängigkeit](./evidence-independence)).

Deshalb behandelt [Kozientist](./coscientist) LLMs als Werkzeuge, nicht als
Orakel. Der [Betreiber](./operator) behält die Souveränität; die
[epistemische Protokollschicht](./epistemic-protocol-layer) erzwingt
[Nachvollziehbarkeit](./traceability) und
[Rebuttal-First-Suche](./rebuttal-first-search); und das
[Multi-AI-Konsensprotokoll](./multi-ai-consensus-protocol) nutzt
Modell-Uneinigkeit als Signal für eine genauere Prüfung. LLMs übernehmen Suche
und Strukturierung; Menschen übernehmen Verifikation und Entscheidung.
