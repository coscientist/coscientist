---
title: مدل زبانی بزرگ
description: "مدل زبانی بزرگ، معماریِ هوش مصنوعیِ زیربنای کارِ تأملِ پژوهشگر مشترک"
sourceLocale: en
sourceHash: a03c9bb62d5e
translatedAt: 2026-01-14
---

LLM (مدل زبانی بزرگ) به مدل‌های شبکهٔ عصبی گفته می‌شود که روی پیکره‌های عظیم متنی آموزش می‌بینند تا زبان طبیعی را پیش‌بینی و تولید کنند. نمونه‌ها شامل GPT، Claude، Gemini و Llama هستند. LLMها می‌توانند طیف گسترده‌ای از وظایف زبانی—خلاصه‌سازی، ترجمه، پرسش‌وپاسخ، تولید کد—را با یادگیری الگوهای آماری از داده‌های آموزشی انجام دهند.

برای [پژوهشگر مشترک](./coscientist)، مدل‌های زبانی بزرگ موتورِ انجامِ [کارِ تأمل](./contemplation-labor) هستند: پیشنهاد فرضیه‌ها، گردآوری شواهد، یافتن نقض‌ها، و ساختاردهیِ استدلال‌ها. چون LLMها می‌توانند هر زبانی را بخوانند، [ترکیبِ میان‌زبانی](./cross-linguistic-synthesis) را به‌عنوان یک قابلیت بومی ممکن می‌کنند.

با این حال، LLMها محدودیت‌های بنیادین دارند. آن‌ها برای «توکنِ بعدیِ محتمل» بهینه می‌شوند، نه برای حقیقت. آن‌ها می‌توانند [توهم‌زایی](./hallucination) کنند: تولید متنی مطمئن و منسجم که از نظر factual (واقعیت‌محور) غلط است. آن‌ها مستعدِ [دامِ روانیِ متن](./fluency-trap) هستند: نثرِ روانی که خطاها را پنهان می‌کند. آن‌ها داده‌های آموزشی مشترک دارند، بنابراین هم‌نظر شدنِ مدل‌ها ممکن است بازتابِ سوگیریِ هم‌بسته باشد، نه [راستی‌آزمایی](./verification) مستقل (نگاه کنید به [استقلالِ شواهد](./evidence-independence)).

به همین دلیل است که [پژوهشگر مشترک](./coscientist) با مدل‌های زبانی بزرگ مانند ابزار رفتار می‌کند، نه مانند وحی. [اپراتور](./operator) حاکمیت را حفظ می‌کند؛ [لایهٔ پروتکل معرفت‌شناختی](./epistemic-protocol-layer) [ردیابی‌پذیری](./traceability) و [جست‌وجوی «ابتدا ردیه»](./rebuttal-first-search) را اعمال می‌کند؛ و [پروتکل اجماعِ چند-هوش‌مصنوعی](./multi-ai-consensus-protocol) از اختلافِ مدل‌ها به‌عنوان سیگنالی برای بررسیِ دقیق‌تر استفاده می‌کند. LLMها جست‌وجو و ساختاربندی را انجام می‌دهند؛ انسان‌ها راستی‌آزمایی و تصمیم‌گیری را.
