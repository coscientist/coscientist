---
title: "ADR 8: Platform Boundaries and Expansion Plan (Convex, Vercel, Workflow, Mastra)"
---

# ADR 8: Platform Boundaries and Expansion Plan (Convex, Vercel, Workflow, Mastra)

## Context

The MVP system is built with a **Next.js frontend on Vercel** and a **Convex backend**. It's important to delineate what each part is responsible for, to avoid confusion and to create a clean architecture that can grow. Additionally, while MVP does not include federated multi-server setups or advanced AI agent features, the architecture should be conceived with a path to those features. This ADR covers how we structure the boundaries between front-end and backend, how we incorporate Convex's special capabilities like scheduling/workflow, and how we plan for eventual introduction of an AI agent system (Mastra) and possibly federated or multi-component architecture.

## Decision

**Maintain a clear client-server separation with Next.js (Vercel) as the frontend/UI layer and Convex as the authoritative backend for data and realtime collaboration, and leverage Convex's workflow and scheduling for background tasks.** The system is initially a single Convex deployment and a single Next.js app, but we outline how to extend beyond that. Key points:

- **Next.js (Vercel) Frontend:**
  - Renders the application UI, handles routing (public routes for pages, maybe protected via Next middleware to ensure login).
  - Uses the Convex React client (ConvexProvider and auto-generated hooks) to call backend functions and subscribe to queries. All data mutations and fetches go through Convex APIs – we avoid maintaining any significant client-side state that isn't also in Convex, except transient UI state.
  - Next.js may also do server-side rendering (SSR) for initial loads, but since most data is behind auth and dynamic, we might rely on client-side rendering after login. If SSR is needed (for public pages or SEO), we can call Convex from getServerSideProps for example to fetch some data (Convex supports Node context).
  - We use Vercel's deployment for fast global delivery of static assets and edge network. But our dynamic data comes from Convex's endpoint (which by default is a single region endpoint, though Convex might have edge caching of queries).
  - Security: We ensure no secret logic is in Next – it's all in Convex functions. Vercel's edge can be used for additional protections (like bot protection, etc., possibly via Vercel's built-in features if needed).
- **Convex Backend:**
  - Houses the database (blocks, edges, steps, presence, etc.), and all business logic in Convex functions (queries, mutations, actions, crons).
  - We treat Convex as the **source of truth** for application state. No other database is used. This centralizes consistency and simplifies sync (Convex keeps clients in sync via reactivity).
  - Realtime collaboration, as described in prior ADRs, is implemented in Convex (e.g., ProseMirror OT functions, presence subscriptions). Convex's **open-source backend platform** keeps the app in sync in realtime[[72]](https://vercel.com/docs/integrations#:~:text=Convex), which is one of the reasons we chose it.
  - We take advantage of Convex **Components** (like Files Control, Prosemirror Sync, etc.) to accelerate development. These are modular backend pieces that integrate with our code.
- **Workflow & Scheduling:** Convex offers a "Workflow" system and cron jobs for background tasks. We use cron for periodic jobs (cleanup expired files, presence trim, etc., as mentioned in earlier ADRs). For more complex or multi-step background tasks, we will use Convex actions (which run without automatically re-running on data change, suitable for one-off tasks). If something long-running is needed (like an AI call that might take many seconds), we can offload that to a Convex async job or an external service but orchestrated via Convex's workflow component if possible. The mention of "Workflow component" implies Convex has durable functions that survive restarts and can handle retries. We will use this for reliability in tasks like sending email notifications or scheduling embargo lifts.
- **Auth:** Likely, we'll use Convex's built-in Auth (which can integrate with Google/GitHub OAuth, etc.). The Next frontend triggers login flows, obtains an ID token, which Convex uses to authenticate. We do not have a separate auth server. This means the client and Convex share user identity context seamlessly.
- **Mastra (AI & Agents) – Future Integration:**
  - Mastra is a TypeScript AI framework (with multi-agent workflow) that Vercel is promoting[[73]](https://vercel.com/docs/ai-gateway/framework-integrations/mastra#:~:text=Mastra%2520is%2520a%2520framework%2520for,model%2520management%2520and%2520routing%2520capabilities). In the future, we plan to incorporate AI "co-scientist" capabilities, possibly by employing Mastra to build agents that can interact with our knowledge base.
  - Our plan is to keep the AI logic **modular**. For example, we might create a separate service or serverless function (could even be on Vercel using the AI SDK) that handles agent reasoning. The agent would communicate with Convex via API (either by calling Convex functions or via Convex HTTP endpoints).
  - We ensure our architecture can accommodate this by: not baking in assumptions that only humans access data. We may have Convex functions that can be triggered by external calls (Convex supports HTTP endpoints or calling functions via API). So an agent could call convex.actions.queryKnowledgeGraph to get info, then decide something, then call a mutation to add a block or edge with its findings.
  - If Mastra is integrated in-process: Possibly, we could run a Mastra agent within a Convex action (since Mastra is just TypeScript, and Convex actions can call external APIs). However, Mastra might have heavier requirements or long-lived processes. Another approach: use Vercel Edge Functions or AWS Lambda to run agents asynchronously, and have them call back to Convex. The specifics will depend on performance. For now, we note that "Mastra is a framework for building AI-powered features on a modern JS stack"[[73]](https://vercel.com/docs/ai-gateway/framework-integrations/mastra#:~:text=Mastra%2520is%2520a%2520framework%2520for,model%2520management%2520and%2520routing%2520capabilities), which suggests it fits well with our Next/Convex stack. We will likely orchestrate via the Vercel AI SDK and ensure our platform boundaries allow it (perhaps treat it as another client or microservice).
- **No Federated Architecture at MVP, but future expansion:**
  - MVP is one Convex deployment (one database, one backend). In future, if we needed multiple backends (for either multi-tenancy or scaling or federation between communities), we'd plan to connect them via an API layer or message passing. For example, one could imagine a scenario where some blocks are hosted in other Convex deployments and referenced remotely. Not in MVP, but our block model (ADR 1) could be extended with a field for origin or global ID to enable references to external content.
  - If needed, we might develop a **federation server** that sits above multiple Convex backends to route requests based on content location or user domain. Alternatively, adopt an open standard like ActivityPub to let different instances follow each other's public content. That would be an extension where our backend exports certain events and consumes from others. The decision is to keep MVP simple (no federation), but we will keep data model flexible enough (like globally unique IDs not assuming a single instance, maybe use UUIDs or something not tied to an auto-increment).
- **DevOps and Deployment:**
  - Vercel and Convex are both hosted services. Deployment is separate: we deploy Next.js to Vercel (which gives us a domain) and deploy Convex functions to Convex cloud. They communicate over the internet. We will secure that by using SSL and by Convex's auth requirement. Also, because Convex functions are like serverless, we trust Convex's scaling.
  - Logging/Monitoring: We rely on Convex's dashboard for backend logs, and Vercel for any front-end logs. If needed we might integrate Sentry or similar for the frontend. Convex likely has some solution for error tracking as well.
- **Edge concerns:** Vercel runs CDN nodes globally for static assets. Convex by default might run in one region (we can pick e.g. US or wherever majority users are). For global users, that might introduce latency on data calls. However, the volume is not high per call, and websockets maintain persistent connections. In future, if needed, Convex might allow multi-region or we could deploy separate Convex instances per region and connect them (non-trivial). We accept some latency tradeoff for simplicity. Perhaps ensure to choose a region that's acceptable (maybe US-east which is a good compromise).
- **Workflow (Convex) usage examples in MVP:**
  - Cron jobs as described (cleanup tasks).
  - Possibly using Convex's durable functions if we have processes like "after a user edits a block, run an AI summarizer in background". We could schedule that via a Convex action with retries.
  - The mention of "Workflow component" in Convex docs suggests an orchestrator for multi-step tasks[[74]](https://stack.convex.dev/presence-with-convex#:~:text=AI%2520Agents%2520with%2520Built). For example, if we integrate an agent that needs to do a sequence: search knowledge, analyze, write result, we could chain those in a durable function to survive restarts.
  - We'll incorporate that approach for any AI or heavy tasks as needed in expansion. For MVP, perhaps not heavily used except scheduled jobs and maybe file cleanup.

## Rationale

- **Separation of Concerns:** By clearly delineating front-end and backend responsibilities, we make the system easier to maintain. Next.js deals with presentation and user interaction, Convex deals with data consistency and business rules. This way, one can change UI without risking backend logic and vice versa.
- **Leverage Specialized Platforms:** Vercel is optimized for frontend delivery (CDN caching, asset optimization, easy deployment), and Convex is optimized for backend state with real-time sync. Using each for what it's best at avoids reinventing wheels. For instance, we didn't attempt to make Next.js do real-time synchronization or host a database – we offload that to Convex which provides a cohesive solution.
- **Scalability and "scale as you grow":** This architecture can start small but grow without major changes. For example, if usage spikes, Convex can scale horizontally behind the scenes, and Vercel will scale our frontend globally (they auto-scale serverless functions and static content). If we need to add more features (like search, AI, etc.), we can often integrate via Convex components or Vercel integrations without overhauling the whole stack.
- **Expansion Readiness:** We consciously consider expansions:
  - **Agents/AIs:** We know that to integrate AI workflows (Mastra), we may need to interface with new services. By keeping our core logic in Convex, we can either call out to AI APIs from Convex or have the AI call into Convex. We avoid putting AI logic in the client because that could expose secrets or be inconsistent. Instead, running it server-side (either in Convex or an allied serverless function) is more secure and reliable. Mastra being a Vercel-integrated framework[[75]](https://vercel.com/docs/ai-sdk#:~:text=AI%2520SDK%2520,servers%2520%C2%B7%2520Vercel%2520MCP) means it likely runs on the server side (or edge side) as well, coordinating with our data.
  - **Federation:** While not needed now, our design of using globally unique block IDs and not assuming a single monolithic server means we could, in future, assign blocks URNs like block:<instance>:<id> to reference remote ones. We haven't explicitly done that yet, but we aren't using things like auto-increment IDs that would clash. Convex uses its own IDs (likely UUIDs or similar) so those could incorporate instance if needed. Also, since all access is via our functions, introducing a federation layer can be done at function level (for example, getBlock might later detect if block not in local DB and then fetch from another server).
- **Monolithic vs Microservices:** We opt for a **monolithic backend** (Convex functions cover everything) rather than splitting microservices for MVP. This reduces complexity early on. However, we acknowledge that adding something like a vector search or heavy ML might be outside Convex's current feature set. In such cases, we can add a microservice just for that piece:
  - For example, if we want semantic search, perhaps we use an external vector DB (Pinecone, etc.). We then have a Convex function call that service. Or we integrate Convex's upcoming vector search if available (Convex has mention of vector search in marketing[[76]](https://stack.convex.dev/presence-with-convex#:~:text=Build%2520in%2520minutes,%2520scale%2520forever)).
  - The plan is to extend within Convex's ecosystem as far as possible (since they are adding components e.g. an AI memory component[[77]](https://stack.convex.dev/presence-with-convex#:~:text=Deployment), vector search, etc.). Only if necessary, spin out separate services.
- **Security Boundary:** By having all sensitive operations on the Convex side, and having Next.js essentially serve static and call APIs, we reduce the risk of exposing secrets. The Next.js environment will hold nothing more than perhaps the Convex deployment URL and a public auth configuration. All secret keys (if any, e.g. for third-party APIs or Mastra integration) will reside in Convex or Vercel serverless function environment variables, not in client code.
- **Consistency:** Running everything through one backend (Convex) ensures consistency in data. It's effectively like having a single database with all constraints and logic in one place. If we had multiple backends (say one for collab, one for files, one for AI), we'd have to synchronize them, which is complex. Using Convex components for various needs (files, etc.) keeps it within one transactional system where possible.
- **Utilizing Vercel/Convex integration:** Vercel has an integration marketplace and Convex is one of the storage/backends listed[[72]](https://vercel.com/docs/integrations#:~:text=Convex). This indicates that Vercel and Convex work smoothly together (e.g., environment config, easy deployments). By sticking to these integrated solutions, we benefit from community and official support.

## Alternatives Considered

- **Single Server (no Convex):** Use a Node.js server (or Next.js API routes) with a database like Postgres or Mongo, and maybe something like ShareDB or Yjs for collab. We did not choose this because it would require building a lot from scratch (real-time sync, OT logic, etc.), and we'd lose Convex's elegant reactivity. Also, scaling that on Vercel might be tricky (WebSockets on Vercel are not straightforward, one might need a separate WebSocket server).
- **Fully Serverless (no dedicated backend):** There was an option to try to do everything with client and maybe some edge functions or a combination of services (e.g., using Firebase for database and auth, maybe a CRDT library for collab). This patchwork would be harder to maintain and might not achieve the tight integration we want (for example, merging CRDT states and ensuring security would be challenging).
- **Integrating AI at MVP baseline:** One thought: include an AI agent from day one. We decided against it to keep MVP focused and not over-complicate initial architecture. We want the architecture to allow adding it, but not necessarily have it now. If we had tried to include it, we might have needed more components (like a vector DB or an external LLM service integration) upfront, which we can postpone.
- **Ignoring expansion concerns:** We could have built MVP ignoring federation or multi-agent future, which might lead to shortcuts that later hinder expansion. We consciously avoided such shortcuts. For example, we use globally unique IDs (not sequential IDs tied to single DB) to ease future merging of data. We structure code so that adding new Convex functions or even new services won't require tearing apart the monolith (e.g., we could factor out AI-related functions to a separate module or even separate deploy if needed).
- **Monorepo vs separate repos:** We considered whether to keep frontend and backend in one repository. Possibly, yes – Convex functions can live in the same codebase as the Next app for convenience (Convex has a dev server that watches file changes). This synergy can be beneficial (shared types between front and back, etc.). On the other hand, logically they are decoupled by API. We think a monorepo is fine (one Vercel project with Convex directory inside, or two projects with shared code). The decision doesn't heavily impact runtime but is about dev experience. We lean monorepo to share code (like types for data models).
- **Third-party integrations early vs late:** There are many Vercel integrations (for logging, auth, etc.). We stick with minimal needed: Convex for backend, maybe Vercel's own AI SDK when time comes. We decided not to integrate something like Sentry from start, but that's easily added if needed for error monitoring. Similarly, no separate analytics yet (maybe just use Vercel/Next analytics or simple logging in Convex).

## Implications

- **Vendor Lock-in:** Choosing Convex and Vercel ties us somewhat to these platforms. If in future we needed to self-host or move to open source stack, we'd have some migration work (Convex has an open source core but not sure if fully self-hostable easily; Vercel is just hosting, we could move to our own Next.js server if needed). We accepted this lock-in trade-off for faster development. We keep our code structured (Convex logic is mostly in functions that could potentially be ported to another server framework if absolutely necessary).
- **Costs:** Both Vercel and Convex have cost models (free tiers and usage-based pricing beyond). We should monitor usage (e.g., Convex function calls, data storage) to avoid surprises. If AI is added, that likely brings API costs too. Having clear boundaries helps here: e.g., we could swap out Convex for an in-house solution if cost demands (not trivial, but contained).
- **Single Point of Failure:** One Convex deployment is one point of failure. If it goes down, the whole app's dynamic features fail. Vercel and Convex both have high availability but not guaranteed 100%. Mitigation in future might be to have a read-only cache or static export of some content for public view if Convex is down. But MVP likely doesn't require that level of redundancy.
- **Development Workflow:** Developers will need to run both Next and Convex dev servers. Fortunately, Convex provides a dev server that runs in background and we can use environment variables to connect Next to that. The integration is decent (just be aware of syncing types).
- **Edge Functions on Vercel:** If we need to run some logic closer to users (like middleware, custom auth redirect, or soon maybe some AI model at edge), we have option to use Vercel Edge Functions. Right now, not heavily used. Possibly for optimizing static site generation or caching public pages, we could do something at edge. But as most content is dynamic and behind login, edge caching is less relevant except maybe caching public read-only content post-embargo.
- **Stateful vs Stateless boundaries:** Convex is stateful (holds DB state). Next/Vercel is stateless between requests (except what's in browser memory). That split means we must be careful to not attempt to store state in Next server memory (which could be ephemeral and multi-instance). All persistent state must go to Convex (or client local state if ephemeral on that client). Also, any server-side rendering should fetch fresh state from Convex rather than assume previous knowledge.
- **Feature development:** When adding new features, we'll decide if it's primarily frontend (just UI changes) or requires backend changes (new Convex functions, tables). We foresee many features involve both. The good thing is our unified language (TypeScript) for both, and likely some shared types (Convex can share types to client via codegen). That accelerates feature dev and reduces mismatch errors.
- **Mastra introduction:** When we actually bring in Mastra, we will allocate possibly a separate set of Convex functions to interface with it. Or maybe run Mastra within an action (depending on how heavy it is). Since Mastra is basically an AI agent framework, if it needs to maintain agent state or run a loop, we might use Convex's persistent storage to store agent memory (they even have an example of "AI Agents with Built-in Memory" on Convex stack[[74]](https://stack.convex.dev/presence-with-convex#:~:text=AI%2520Agents%2520with%2520Built)). That suggests synergy: agent's memory (like conversation logs or knowledge) stored in Convex, agent logic runs in either Convex actions or in Vercel functions using the Vercel AI SDK and calls Convex to fetch memory or update results.
- **Workflow Orchestration Example:** Suppose we have a feature "Summarize all evidence for this claim". Implementation might be: Convex action fetches relevant blocks (maybe heavy query across edges), sends the text to an LLM API (like OpenAI) for summarization, gets result, inserts a new block with summary. This sequence can fail at API call or take time. Using Convex's workflow (durable function with retry) ensures it will complete even if there's a transient failure. This is how we plan to use workflow, ensuring reliability for such tasks. That's beyond core MVP, but may appear as soon as we integrate AI.
- **APIs for external integration:** If we or others want to integrate with this knowledge system from outside (say, an external script or service needs to query data or add a block), how to do it? Convex provides an HTTP API (Convex HTTP endpoints or using API keys to call functions). We likely will expose some endpoints for integration. This foresight means we'll design Convex functions with clean interfaces that could later be exposed externally (with appropriate auth, maybe using API keys or service accounts). For example, in a federated scenario or a plugin ecosystem, we might let external tools call createBlock or queryEdges.
- **Logging and Monitoring in Production:** As we expand, monitoring performance and errors becomes crucial. Convex provides some telemetry, and Vercel provides logs for serverless function usage. We may add instrumentation (e.g., measure how long collab merges take, etc.). If something becomes a bottleneck (like a particular query slow as data grows), we'll consider caching or indexing more. At least in design, we keep heavy loops out of query functions (Convex queries should ideally just fetch by indexes, not do crazy compute).
- **Continuous Deployment:** We intend to deploy often. Vercel and Convex both support CI/CD style deployment. They may not automatically coordinate (i.e., deploying new Convex schema and new frontend simultaneously), so we'll manage that (maybe deploy Convex first, ensure functions are updated, then deploy frontend that calls them). In local dev, we test changes to both. This is manageable but something to watch when doing migrations (like changing Convex schema might need careful roll-out).
- **Documentation for Devs:** We'll document these boundaries for any new developers: e.g., "All data mutations go in Convex functions. Do not attempt to mutate application state in Next.js side. Next.js should only call Convex or render UI." Also, "If adding a new background job, consider using Convex cron or action rather than building a separate scheduler."

## Expansion Path

See integration sections in each previous ADR for feature-specific expansions. Here we note architectural expansions:

- **Federation:** If we allow federated architecture (multiple servers or instances sharing knowledge), the block model is well-suited to become the unit of federation. Each block can carry a global identifier (possibly a composite of instance + ID) to be referenced across instances. We would need a mechanism to sync or fetch remote blocks on demand in a federated scenario, and a way to resolve conflicts if the same block is edited in different instances (which might entail CRDTs or consensus, see ADR 2 and 3 expansions). For federation, we might develop a layer above our current backend that routes to the correct instance (like a smart proxy or even an ActivityPub-compatible server that federates public content).
- **AI and Agents (Mastra):** Expand to include AI agents that can read and contribute to the knowledge base. We will integrate Mastra for multi-agent workflows. Agents may have their own memory (stored in Convex), and they can invoke our functions to query or update data. Possibly, agents run in background processes orchestrated by workflows. For example, an agent could be triggered on new content to suggest links or summarize. The architecture supports this by treating agents as special users or processes that have API access. We'll need to consider agent permissions (ADR 6 expansion) and how agents interact with collaborative features.
- **Vector Search and Semantic Search:** If we integrate a vector search system for finding similar content, we might use Convex's vector search if available or integrate an external service like Pinecone. This would be called from our backend functions. For example, a search query function might convert the query to an embedding via an AI API, then query the vector DB, and return matching block IDs. Then retrieve those blocks from Convex. This combines AI (for embeddings) and specialized DBs (vector DB) with our existing structure. The architecture can accommodate by calling out to those services from Convex functions or from a dedicated service (which then calls back to fetch block details).
- **Public Publishing and Static Exports:** For any blocks marked public (ADR 6), we might want to export them to a static site (like a static blog). We could have a process that, periodically or on trigger, pulls all public blocks from Convex and generates Next.js static pages or even a separate static site. This would enable SEO and serve public content without hitting Convex for reads. The architecture allows this since we can query all public blocks and render them. If using federation, maybe different instances export to different domains, and they cross-link.
- **Offline and Local-First:** If we ever want offline editing, we might integrate a CRDT approach (like Yjs, see ADR 2 & 3 expansion). Alternatively, use Convex's local-first capabilities if they expand. This would likely mean a significant architecture shift (client-side database that syncs when online), but our block model could still apply (blocks would be synced to local storage as well).
- **Workflow and Automation Enhancements:** We might implement user-defined workflows in the system. For example, a user could create a rule "when a claim block is added, run an agent to find supporting evidence". This would be a workflow engine (possibly Mastra or Convex's own) that fires on events. The architecture would need a trigger system (like Convex triggers on table changes to start a background job). This is speculative, but our setup allows it.
- **Multi-Tenancy or Workspaces:** If we want to support multiple organizations on one Convex instance, we'd need to partition data (add a tenantId or workspaceId to blocks, edges, etc.) and ensure queries filter by that. Permissions would also tenant-scope (ACL can include the tenant context). This is not in MVP but if the product scales to SaaS multi-tenant, the architecture can adapt. Convex's single deployment can serve multiple tenants as long as we partition correctly. We might also consider separate Convex deployments per tenant if needed for isolation (at a larger scale or for security).
