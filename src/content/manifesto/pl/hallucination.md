---
title: Halucynacja
description:
  Wyniki AI, które brzmią wiarygodnie, ale są faktograficznie niepoprawne lub
  zmyślone
sourceLocale: en
sourceHash: 35a042b51f6a
translatedAt: 2026-01-14
---

Halucynacja to wynik AI, który brzmi pewnie i spójnie, ale zawiera błędy
faktograficzne, zmyślone informacje lub wymyślone źródła. Niebezpieczeństwo nie
polega na losowości: polega na wiarygodności. Zhalucynowany tekst często
przechodzi pobieżną weryfikację, ponieważ podąża za statystycznymi wzorcami
prawdziwej prozy.

Halucynacja jest objawem tego, jak działają [LLM-y](./llm): przewidują
prawdopodobne kolejne tokeny, a nie prawdziwe. Gdy dane treningowe na dany temat
są skąpe lub sprzeczne, model interpoluje, a efektem może być płynnie brzmiący
błąd. Dlatego [pułapka płynności](./fluency-trap) jest tak niebezpieczna:
płynność nie oznacza poprawności.

W [Współnaukowiec](./coscientist) ryzyko halucynacji jest zarządzane poprzez
[warstwę protokołów epistemicznych](./epistemic-protocol-layer):
[możliwość prześledzenia](./traceability) wymaga, by twierdzenia łączyły się z
[fragmentami dowodów](./evidence-span),
[wyszukiwanie z priorytetem obalenia](./rebuttal-first-search) poddaje je testom
wytrzymałościowym przed akceptacją, a
[Protokół Konsensusu Multi-AI](./multi-ai-consensus-protocol) wykorzystuje
niezgodność modeli jako sygnał do dokładniejszej kontroli.
