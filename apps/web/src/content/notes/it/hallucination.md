---
title: Allucinazione
description: Output dell’IA che sono plausibili ma fattualmente errati o inventati
sourceLocale: en
sourceHash: 35a042b51f6a
translatedAt: 2026-01-14
---

Un’allucinazione è un output dell’IA che appare sicuro e coerente, ma contiene
errori fattuali, informazioni fabbricate o riferimenti inventati. Il pericolo
non è la casualità: è la plausibilità. Il testo allucinato spesso supera un
controllo superficiale perché segue i pattern statistici della prosa veritiera.

L’allucinazione è un sintomo di come funzionano gli [LLM](./llm): prevedono i
token successivi probabili, non quelli veri. Quando i dati di addestramento sono
scarsi o contraddittori su un tema, il modello interpola e il risultato può
essere elegantemente sbagliato. Ecco perché la
[trappola della fluidità](./fluency-trap) è così pericolosa: la fluidità non
implica accuratezza.

In [Coscienziato](./coscientist), il rischio di allucinazione è gestito tramite
il [livello di protocollo epistemico](./epistemic-protocol-layer): la
[tracciabilità](./traceability) richiede che le affermazioni siano collegate a
[estratti di evidenza](./evidence-span), la
[ricerca "confutazione prima"](./rebuttal-first-search) mette sotto stress le
ipotesi prima dell’accettazione, e il
[Protocollo di Consenso Multi-AI](./multi-ai-consensus-protocol) usa il
disaccordo tra modelli come segnale per un esame più attento.
