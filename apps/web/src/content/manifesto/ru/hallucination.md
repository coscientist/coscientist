---
title: Галлюцинация
description:
  Выводы ИИ, которые выглядят правдоподобно, но фактически неверны или
  сфабрикованы
sourceLocale: en
sourceHash: 35a042b51f6a
translatedAt: 2026-01-14
---

Галлюцинация — это вывод ИИ, который звучит уверенно и связно, но содержит
фактические ошибки, сфабрикованную информацию или выдуманные ссылки на
источники. Опасность не в случайности, а в правдоподобии. Галлюцинированный
текст часто проходит поверхностную проверку, потому что следует статистическим
паттернам правдивой прозы.

Галлюцинация — симптом того, как работают [LLMs](./llm): они предсказывают
вероятные следующие токены, а не истинные. Когда обучающие данные по теме
разрежены или противоречивы, модель интерполирует, и результат может быть гладко
неверным. Поэтому [ловушка беглости](./fluency-trap) так опасна: беглость не
подразумевает точность.

В [Соисследователь](./coscientist) риск галлюцинаций управляется через
[слой эпистемического протокола](./epistemic-protocol-layer):
[трассируемость](./traceability) требует, чтобы утверждения были связаны с
[фрагментами доказательств](./evidence-span),
[поиск сначала опровержений](./rebuttal-first-search) проводит стресс-тест перед
принятием, а [Протокол мульти-ИИ консенсуса](./multi-ai-consensus-protocol)
использует расхождения между моделями как сигнал к более тщательной проверке.
